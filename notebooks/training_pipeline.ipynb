{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Similarity Search Engine - Training Pipeline\n",
    "\n",
    "This notebook covers the data engineering and modeling phases:\n",
    "1.  **Data Loading**: Loading images and labels.\n",
    "2.  **Feature Extraction**: Using MobileNetV2 to extract features.\n",
    "3.  **Dimensionality Reduction**: Using PCA.\n",
    "4.  **Clustering**: K-Means with Elbow Method.\n",
    "5.  **Classification**: Linear SVM.\n",
    "6.  **Search Index**: Building a FAISS index.\n",
    "7.  **Evaluation**: Visualizing and measuring performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import faiss\n",
    "\n",
    "# Config\n",
    "DATA_DIR = '../data/images'\n",
    "CSV_PATH = '../data/images_dataset.csv'\n",
    "MODELS_DIR = '../models'\n",
    "IMG_SIZE = (224, 224)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Metadata\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Extract filename from URL (assuming filenames in 'downloaded_images' match the end of the URL)\n",
    "# Use a more robust check if needed. For now, let's assume standard Amazon format.\n",
    "def extract_filename(url):\n",
    "    return url.split('/')[-1]\n",
    "\n",
    "df['filename'] = df['image'].apply(extract_filename)\n",
    "\n",
    "# Filter for images that actually exist\n",
    "existing_files = set(os.listdir(DATA_DIR))\n",
    "df = df[df['filename'].isin(existing_files)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Found {len(df)} images available locally out of {len(pd.read_csv(CSV_PATH))}.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction (MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained Model\n",
    "mobilenet = MobileNetV2(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "mobilenet.trainable = False\n",
    "\n",
    "def load_and_preprocess_image(filename):\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    img = keras_image.load_img(path, target_size=IMG_SIZE)\n",
    "    x = keras_image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "# Extract Features\n",
    "# Using batch processing for efficiency is possible, but loop is simpler for tutorial notebooks\n",
    "features = []\n",
    "valid_indices = []\n",
    "\n",
    "print(\"Extracting features...\")\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    try:\n",
    "        img_data = load_and_preprocess_image(row['filename'])\n",
    "        feat = mobilenet.predict(img_data, verbose=0)\n",
    "        features.append(feat.flatten())\n",
    "        valid_indices.append(i)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['filename']}: {e}\")\n",
    "\n",
    "features = np.array(features)\n",
    "df_clean = df.iloc[valid_indices].reset_index(drop=True)\n",
    "print(f\"Feature matrix shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA Compression & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA\n",
    "pca = PCA(n_components=50)\n",
    "pca_features = pca.fit_transform(features)\n",
    "\n",
    "# Visualization of Variance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Cumulative variance explained by 50 components: {np.sum(pca.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering (K-Means) with Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "k_range = range(2, 16)\n",
    "\n",
    "print(\"Running Elbow Method...\")\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=SEED, n_init='auto')\n",
    "    kmeans.fit(pca_features)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_range, wcss, marker='o')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Final K-Means (Let's pick an optimal K, e.g., 6, or inspect the plot above first)\n",
    "# For automation, let's assume K=6 is reasonable for this dataset size/diversity, or user sets it.\n",
    "OPTIMAL_K = 6 # Update this based on the plot\n",
    "kmeans = KMeans(n_clusters=OPTIMAL_K, random_state=SEED, n_init='auto')\n",
    "clusters = kmeans.fit_predict(pca_features)\n",
    "\n",
    "df_clean['cluster'] = clusters\n",
    "\n",
    "# Visualize Clusters (2D PCA)\n",
    "pca_2d = PCA(n_components=2)\n",
    "features_2d = pca_2d.fit_transform(features)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=features_2d[:,0], y=features_2d[:,1], hue=clusters, palette='tab10', s=50, alpha=0.7)\n",
    "plt.title('K-Means Clusters (2D PCA)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification (Linear SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data using CSV Labels\n",
    "# Mapping category text to int\n",
    "target_column = 'main_category' # Or 'name' if we want granular, but 'main_category' is likely better for SVM accuracy\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_clean[target_column])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(pca_features, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "\n",
    "# Train SVM\n",
    "svm_model = LinearSVC(random_state=SEED, dual='auto')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - SVM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FAISS Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Index\n",
    "d = pca_features.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(pca_features.astype('float32'))\n",
    "\n",
    "print(f\"Number of vectors in index: {index.ntotal}\")\n",
    "\n",
    "# Save Index\n",
    "faiss.write_index(index, os.path.join(MODELS_DIR, 'faiss_index.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODELS_DIR, 'pca.pkl'), 'wb') as f:\n",
    "    pickle.dump(pca, f)\n",
    "\n",
    "with open(os.path.join(MODELS_DIR, 'kmeans.pkl'), 'wb') as f:\n",
    "    pickle.dump(kmeans, f)\n",
    "\n",
    "with open(os.path.join(MODELS_DIR, 'svm.pkl'), 'wb') as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "\n",
    "with open(os.path.join(MODELS_DIR, 'label_encoder.pkl'), 'wb') as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "# Save Processed Metadata with PCA features (optional, for app fast loading)\n",
    "df_clean.to_pickle(os.path.join(DATA_DIR, 'processed_metadata.pkl'))\n",
    "\n",
    "print(\"All models and data saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
